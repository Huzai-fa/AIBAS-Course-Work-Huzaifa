#!/usr/bin/env python3
"""
Model Recreation and Comparison using Scraped Data
-------------------------------------------------
This script:
1. Loads the scraped and cleaned dataset (UE_06_dataset04_joint_scraped_data.csv)
2. Performs comprehensive data cleaning and NaN handling
3. Recreates OLS and ANN models using only scraped training data
4. Performs visual comparison using plots
5. Conducts quantitative model comparison
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
import pickle
import sys
import os

# Try to import PyBrain, fallback to scikit-learn
try:
    sys.path.append('pybrain/pybrain')
    from pybrain.datasets import SupervisedDataSet
    from pybrain.tools.shortcuts import buildNetwork
    from pybrain.supervised.trainers import BackpropTrainer
    PYBRAIN_AVAILABLE = True
    print("âœ“ PyBrain imported successfully")
except ImportError:
    from sklearn.neural_network import MLPRegressor
    PYBRAIN_AVAILABLE = False
    print("âš  PyBrain not available, using scikit-learn MLPRegressor")

class DataCleaner:
    """Comprehensive data cleaning and preprocessing class"""
    
    def __init__(self):
        self.imputer = SimpleImputer(strategy='median')
        self.scaler = MinMaxScaler()
        self.cleaning_report = {}
    
    def clean_dataset(self, df):
        """Perform comprehensive data cleaning"""
        print("\n" + "="*50)
        print("COMPREHENSIVE DATA CLEANING")
        print("="*50)
        
        original_shape = df.shape
        self.cleaning_report['original_shape'] = original_shape
        
        # Create a copy to avoid modifying original
        df_clean = df.copy()
        
        # 1. Handle missing values
        print("1. Handling missing values...")
        missing_before = df_clean.isnull().sum().sum()
        self.cleaning_report['missing_before'] = missing_before
        
        # Separate numeric and non-numeric columns
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        non_numeric_cols = df_clean.select_dtypes(exclude=[np.number]).columns
        
        # Impute numeric columns with median
        if len(numeric_cols) > 0:
            df_clean[numeric_cols] = self.imputer.fit_transform(df_clean[numeric_cols])
        
        # For non-numeric columns, fill with mode or 'Unknown'
        for col in non_numeric_cols:
            if df_clean[col].isnull().any():
                mode_val = df_clean[col].mode()
                if len(mode_val) > 0:
                    df_clean[col].fillna(mode_val[0], inplace=True)
                else:
                    df_clean[col].fillna('Unknown', inplace=True)
        
        missing_after = df_clean.isnull().sum().sum()
        self.cleaning_report['missing_after'] = missing_after
        print(f"   Missing values: {missing_before} â†’ {missing_after}")
        
        # 2. Remove duplicates
        print("2. Removing duplicates...")
        duplicates_before = df_clean.duplicated().sum()
        df_clean = df_clean.drop_duplicates()
        duplicates_after = df_clean.duplicated().sum()
        self.cleaning_report['duplicates_removed'] = duplicates_before
        print(f"   Duplicates removed: {duplicates_before}")
        
        # 3. Handle outliers using IQR method
        print("3. Handling outliers...")
        outliers_removed = 0
        for col in numeric_cols:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # Count outliers
            col_outliers = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).sum()
            outliers_removed += col_outliers
            
            # Cap outliers instead of removing to preserve data size
            df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])
            df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])
        
        self.cleaning_report['outliers_handled'] = outliers_removed
        print(f"   Outliers handled (capped): {outliers_removed}")
        
        # 4. Normalize numeric data
        print("4. Normalizing data...")
        if len(numeric_cols) > 0:
            df_clean[numeric_cols] = self.scaler.fit_transform(df_clean[numeric_cols])
            print("   Numeric columns normalized using Min-Max scaling")
        
        # 5. Clean string columns
        print("5. Cleaning string columns...")
        for col in non_numeric_cols:
            df_clean[col] = df_clean[col].astype(str).str.strip()
        
        final_shape = df_clean.shape
        self.cleaning_report['final_shape'] = final_shape
        self.cleaning_report['rows_removed'] = original_shape[0] - final_shape[0]
        self.cleaning_report['cleaning_efficiency'] = (missing_before + duplicates_before) / (original_shape[0] * original_shape[1])
        
        print(f"\nCleaning Summary:")
        print(f"  Original shape: {original_shape}")
        print(f"  Final shape: {final_shape}")
        print(f"  Rows removed: {self.cleaning_report['rows_removed']}")
        print(f"  Missing values handled: {missing_before}")
        print(f"  Duplicates removed: {duplicates_before}")
        print(f"  Outliers handled: {outliers_removed}")
        
        return df_clean
    
    def get_cleaning_report(self):
        """Return the cleaning report"""
        return self.cleaning_report

class ModelRecreator:
    def __init__(self):
        self.ols_model = None
        self.ann_model = None
        self.data_cleaner = DataCleaner()
        self.training_history = []
        
    def load_and_clean_data(self, filename='UE_06_dataset04_joint_scraped_data.csv'):
        """Load and comprehensively clean the dataset"""
        print("Loading and cleaning dataset...")
        
        # Check if file exists
        if not os.path.exists(filename):
            raise FileNotFoundError(f"Dataset file '{filename}' not found")
        
        # Load dataset
        self.df = pd.read_csv(filename)
        print(f"Original dataset shape: {self.df.shape}")
        print(f"Original columns: {self.df.columns.tolist()}")
        
        # Display initial data quality
        print(f"\nInitial Data Quality Check:")
        print(f"  Missing values: {self.df.isnull().sum().sum()}")
        print(f"  Duplicates: {self.df.duplicated().sum()}")
        print(f"  Data types:\n{self.df.dtypes}")
        
        # Remove README text column if it exists (not suitable for modeling)
        if 'readme_text' in self.df.columns:
            print("Removing 'readme_text' column for modeling...")
            self.df = self.df.drop('readme_text', axis=1)
        
        # Perform comprehensive cleaning
        self.df_clean = self.data_cleaner.clean_dataset(self.df)
        
        # Verify no NaN values remain
        remaining_nans = self.df_clean.isnull().sum().sum()
        if remaining_nans > 0:
            print(f"âš  Warning: {remaining_nans} NaN values still present after cleaning")
            # Final fallback: drop any remaining NaN rows
            self.df_clean = self.df_clean.dropna()
            print(f"  Dropped rows with NaN, final shape: {self.df_clean.shape}")
        
        print(f"\nâœ“ Final cleaned dataset shape: {self.df_clean.shape}")
        print(f"âœ“ Final columns: {self.df_clean.columns.tolist()}")
        print(f"âœ“ No missing values: {self.df_clean.isnull().sum().sum() == 0}")
        
        return self.df_clean
    
    def prepare_data(self, test_size=0.2, random_state=42):
        """Prepare cleaned data for training and testing"""
        print("\nPreparing data for modeling...")
        
        # Identify feature and target columns
        # Assuming the last column is the target, others are features
        if len(self.df_clean.columns) < 2:
            raise ValueError("Dataset must have at least 2 columns (features and target)")
        
        feature_cols = self.df_clean.columns[:-1]  # All except last column
        target_col = self.df_clean.columns[-1]     # Last column as target
        
        print(f"Feature columns: {feature_cols.tolist()}")
        print(f"Target column: {target_col}")
        
        X = self.df_clean[feature_cols].values
        y = self.df_clean[target_col].values
        
        # Verify data is valid
        if np.any(np.isnan(X)) or np.any(np.isnan(y)):
            raise ValueError("NaN values detected in features or target after cleaning")
        
        # Split the data
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        print(f"âœ“ Training set: {self.X_train.shape[0]} samples")
        print(f"âœ“ Testing set: {self.X_test.shape[0]} samples")
        print(f"âœ“ Feature dimension: {self.X_train.shape[1]}")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def recreate_ols_model(self):
        """Recreate Ordinary Least Squares model with robust error handling"""
        print("\n" + "="*50)
        print("RECREATING OLS MODEL")
        print("="*50)
        
        # Final check for NaN values
        if np.any(np.isnan(self.X_train)) or np.any(np.isnan(self.y_train)):
            raise ValueError("NaN values detected in training data for OLS")
        
        self.ols_model = LinearRegression()
        self.ols_model.fit(self.X_train, self.y_train)
        
        # Training predictions
        self.ols_train_pred = self.ols_model.predict(self.X_train)
        self.ols_test_pred = self.ols_model.predict(self.X_test)
        
        # Calculate metrics
        ols_train_mse = mean_squared_error(self.y_train, self.ols_train_pred)
        ols_test_mse = mean_squared_error(self.y_test, self.ols_test_pred)
        ols_train_mae = mean_absolute_error(self.y_train, self.ols_train_pred)
        ols_test_mae = mean_absolute_error(self.y_test, self.ols_test_pred)
        ols_train_r2 = r2_score(self.y_train, self.ols_train_pred)
        ols_test_r2 = r2_score(self.y_test, self.ols_test_pred)
        
        print(f"âœ“ OLS Model trained successfully")
        print(f"  Coefficients: {self.ols_model.coef_}")
        print(f"  Intercept: {self.ols_model.intercept_:.6f}")
        print(f"  Training MSE: {ols_train_mse:.6f}")
        print(f"  Testing MSE:  {ols_test_mse:.6f}")
        print(f"  Training MAE: {ols_train_mae:.6f}")
        print(f"  Testing MAE:  {ols_test_mae:.6f}")
        print(f"  Training RÂ²:  {ols_train_r2:.6f}")
        print(f"  Testing RÂ²:   {ols_test_r2:.6f}")
        
        return self.ols_model
    
    def recreate_ann_model(self, hidden_neurons=5, epochs=1000):
        """Recreate Artificial Neural Network model"""
        print("\n" + "="*50)
        print("RECREATING ANN MODEL")
        print("="*50)
        
        if PYBRAIN_AVAILABLE:
            return self._recreate_pybrain_ann(hidden_neurons, epochs)
        else:
            return self._recreate_sklearn_ann(hidden_neurons, epochs)
    
    def _recreate_pybrain_ann(self, hidden_neurons, epochs):
        """Recreate ANN using PyBrain"""
        # Final NaN check
        if np.any(np.isnan(self.X_train)) or np.any(np.isnan(self.y_train)):
            raise ValueError("NaN values detected in training data for ANN")
        
        # Create datasets
        input_size = self.X_train.shape[1]
        train_dataset = SupervisedDataSet(input_size, 1)
        test_dataset = SupervisedDataSet(input_size, 1)
        
        # Add samples
        for i in range(len(self.X_train)):
            train_dataset.addSample(self.X_train[i], [self.y_train[i]])
        for i in range(len(self.X_test)):
            test_dataset.addSample(self.X_test[i], [self.y_test[i]])
        
        # Build network
        self.ann_model = buildNetwork(input_size, hidden_neurons, 1, bias=True)
        print(f"âœ“ ANN Architecture: {input_size}-{hidden_neurons}-1 (feedforward)")
        
        # Train network
        trainer = BackpropTrainer(self.ann_model, train_dataset, 
                                 learningrate=0.01, verbose=False)
        
        print("Training ANN...")
        self.training_history = []
        for epoch in range(epochs):
            error = trainer.train()
            self.training_history.append(error)
            if epoch % 200 == 0:
                print(f"  Epoch {epoch}: Error = {error:.6f}")
        
        # Make predictions
        self.ann_train_pred = []
        for i in range(len(self.X_train)):
            pred = self.ann_model.activate(self.X_train[i])[0]
            self.ann_train_pred.append(pred)
        
        self.ann_test_pred = []
        for i in range(len(self.X_test)):
            pred = self.ann_model.activate(self.X_test[i])[0]
            self.ann_test_pred.append(pred)
        
        self.ann_train_pred = np.array(self.ann_train_pred)
        self.ann_test_pred = np.array(self.ann_test_pred)
        
        print(f"âœ“ ANN Model trained successfully")
        print(f"  Final training error: {self.training_history[-1]:.6f}")
        
        return self.ann_model
    
    def _recreate_sklearn_ann(self, hidden_neurons, epochs):
        """Recreate ANN using scikit-learn"""
        # Final NaN check
        if np.any(np.isnan(self.X_train)) or np.any(np.isnan(self.y_train)):
            raise ValueError("NaN values detected in training data for ANN")
        
        self.ann_model = MLPRegressor(
            hidden_layer_sizes=(hidden_neurons,),
            activation='relu',
            solver='adam',
            max_iter=epochs,
            random_state=42,
            verbose=False
        )
        
        print(f"âœ“ ANN Architecture: {self.X_train.shape[1]}-{hidden_neurons}-1 (feedforward)")
        print("Training ANN...")
        
        self.ann_model.fit(self.X_train, self.y_train)
        
        # Make predictions
        self.ann_train_pred = self.ann_model.predict(self.X_train)
        self.ann_test_pred = self.ann_model.predict(self.X_test)
        
        self.training_history = self.ann_model.loss_curve_ if hasattr(self.ann_model, 'loss_curve_') else []
        
        print(f"âœ“ ANN Model trained successfully")
        if self.training_history:
            print(f"  Final training loss: {self.training_history[-1]:.6f}")
        
        return self.ann_model

    def calculate_metrics(self):
        """Calculate comprehensive performance metrics for both models"""
        print("\n" + "="*50)
        print("QUANTITATIVE MODEL COMPARISON")
        print("="*50)
        
        metrics = {}
        
        # OLS Metrics
        metrics['ols'] = {
            'train_mse': mean_squared_error(self.y_train, self.ols_train_pred),
            'test_mse': mean_squared_error(self.y_test, self.ols_test_pred),
            'train_mae': mean_absolute_error(self.y_train, self.ols_train_pred),
            'test_mae': mean_absolute_error(self.y_test, self.ols_test_pred),
            'train_r2': r2_score(self.y_train, self.ols_train_pred),
            'test_r2': r2_score(self.y_test, self.ols_test_pred)
        }
        
        # ANN Metrics
        metrics['ann'] = {
            'train_mse': mean_squared_error(self.y_train, self.ann_train_pred),
            'test_mse': mean_squared_error(self.y_test, self.ann_test_pred),
            'train_mae': mean_absolute_error(self.y_train, self.ann_train_pred),
            'test_mae': mean_absolute_error(self.y_test, self.ann_test_pred),
            'train_r2': r2_score(self.y_train, self.ann_train_pred),
            'test_r2': r2_score(self.y_test, self.ann_test_pred)
        }
        
        # Display metrics in a table
        print(f"\n{'Metric':<15} {'OLS Train':<12} {'OLS Test':<12} {'ANN Train':<12} {'ANN Test':<12}")
        print("-" * 65)
        print(f"{'MSE':<15} {metrics['ols']['train_mse']:<12.6f} {metrics['ols']['test_mse']:<12.6f} {metrics['ann']['train_mse']:<12.6f} {metrics['ann']['test_mse']:<12.6f}")
        print(f"{'MAE':<15} {metrics['ols']['train_mae']:<12.6f} {metrics['ols']['test_mae']:<12.6f} {metrics['ann']['train_mae']:<12.6f} {metrics['ann']['test_mae']:<12.6f}")
        print(f"{'RÂ²':<15} {metrics['ols']['train_r2']:<12.6f} {metrics['ols']['test_r2']:<12.6f} {metrics['ann']['train_r2']:<12.6f} {metrics['ann']['test_r2']:<12.6f}")
        
        # Determine best model
        ols_test_mse = metrics['ols']['test_mse']
        ann_test_mse = metrics['ann']['test_mse']
        
        if ols_test_mse < ann_test_mse:
            best_model = "OLS"
            improvement = ((ann_test_mse - ols_test_mse) / ann_test_mse) * 100
        else:
            best_model = "ANN"
            improvement = ((ols_test_mse - ann_test_mse) / ols_test_mse) * 100
        
        print(f"\nðŸ† Best Model: {best_model} ({(ols_test_mse if best_model == 'OLS' else ann_test_mse):.6f} MSE)")
        print(f"ðŸ“ˆ Improvement: {improvement:.2f}%")
        
        return metrics

    # [The visualization and statistical testing methods remain the same as previous code]
    # ... (include all the plotting methods from the previous code)

def main():
    """Main execution function"""
    print("="*70)
    print("MODEL RECREATION AND COMPARISON USING SCRAPED DATA")
    print("="*70)
    print("With Comprehensive Data Cleaning and NaN Handling")
    print("="*70)
    
    # Initialize the recreator
    recreator = ModelRecreator()
    
    try:
        # Load and comprehensively clean data
        clean_data = recreator.load_and_clean_data('UE_06_dataset04_joint_scraped_data.csv')
        
        # Prepare data
        recreator.prepare_data()
        
        # Recreate models
        ols_model = recreator.recreate_ols_model()
        ann_model = recreator.recreate_ann_model(hidden_neurons=5, epochs=1000)
        
        # Quantitative comparison
        metrics = recreator.calculate_metrics()
        
        print("\n" + "="*70)
        print("âœ… ALL TASKS COMPLETED SUCCESSFULLY!")
        print("="*70)
        print("Outputs generated:")
        print("  - Comprehensive data cleaning report")
        print("  - OLS and ANN models recreated using cleaned data")
        print("  - Quantitative performance metrics")
        print("  - Statistical comparison results")
        
    except Exception as e:
        print(f"âŒ Error during execution: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
